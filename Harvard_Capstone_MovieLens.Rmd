---
title: "Capstone_HarvardX_Project_MovieLens_10M_Dataset_VEDANT PATIL"
output: pdf_document
---
1. INTRODUCTION :

In order to decrease the human effort and to ease the process of suggesting or recommending items, things,
ideas etc, machine learning was introduced. Traditionally, recommendations on E-Commerce platforms were sent by E-mails or messages which was a time consuming and tedious process. Thus, further technological advancements in the field of Machine Learning minimized these efforts and big giants such as Netflix, Amazon, Google etc adapted their own recommendation engines. 

  1.1 Dataset Description:
                          This dataset is a subset of a large dataset MovieLens generated by Groplens Research lab. This dataset comprises of 6 features and 9,000,055 observations. The validation set which represents 10% of the 10M Movielens dataset contains the same features, but with a total of 999,999 occurences. we made sure that userId and movieId in edx set are also in validation set.
  Each row represents a rating given by one user to one movie. The column “rating” is the outcome we want to predict, y. Taking into account both datasets, here are the features and their characteristics:

quantitative features 

-userId : discrete, Unique ID for the user.

-movieId: discrete, Unique ID for the movie.

-timestamp : discrete , Date and time the rating was given.

qualitative features

-title: nominal , movie title (not unique)

-genres: nominal, genres associated with the movie.

outcome,y

-rating : continuous, a rating between 0 and 5 for the movie.

The goal of the project is to build a recommendation engine with lowest RMSE(ROOT MEAN SQUARE ERROR). Also, the key steps that are performed to build the alogrithm are : Data Extraction, Data cleaning and Data filtering. Data filtering process is performed after collecting ans storing the data to extract the relevant information required to make the final recommendations.

This project adapts Matrix factorization with Stochastic Gradient Descent method which is considered the popular technique to solve recommender system problems.




```{r}
#Installing required packages and calling the specific libraries

library(markdown)
library(knitr)
library(lubridate)
library(recosystem)
library(irlba)
library(Matrix.utils)
library(ggplot2)
library(tidyr)
library(DT)
library(wordcloud)
library(dplyr)
library(tidyverse)
library(caret)
library(data.table)

# MovieLens 10M dataset:
# https://grouplens.org/datasets/movielens/10m/
# http://files.grouplens.org/datasets/movielens/ml-10m.zip

dl <- tempfile()
download.file("http://files.grouplens.org/datasets/movielens/ml-10m.zip", dl)

ratings <- fread(text = gsub("::", "\t", readLines(unzip(dl, "ml-10M100K/ratings.dat"))),
                 col.names = c("userId", "movieId", "rating", "timestamp"))

movies <- str_split_fixed(readLines(unzip(dl, "ml-10M100K/movies.dat")), "\\::", 3)
colnames(movies) <- c("movieId", "title", "genres")
movies <- as.data.frame(movies) %>% mutate(movieId = as.numeric(movieId),
         title = as.character(title),
         genres = as.character(genres))

movielens <- left_join(ratings, movies, by = "movieId")

# 'Validation' set will be 10% of MovieLens data
set.seed(1, sample.kind="Rounding")
test_index <- createDataPartition(y = movielens$rating, times = 1, p = 0.1, list = FALSE)
edx <- movielens[-test_index,]
temp <- movielens[test_index,]

# Make sure userId and movieId in 'validation' set are also in 'edx' set
validation <- temp %>% 
  semi_join(edx, by = "movieId") %>%
  semi_join(edx, by = "userId")

# Add rows removed from 'validation' set back into 'edx' set
removed <- anti_join(temp, validation)
edx <- rbind(edx, removed)

rm(dl, ratings, movies, test_index, temp, movielens, removed)
```

2. Data Exploration : The best ways to know any dataset is performed by automated activities which includes data profiling or data visualization or tabular reports.
Here, the researcher has made detailed analysis of the dataset through visualizations by taking each variable into consideration.
```{r}
#2.1 Data Analysis 
str(edx)
head(edx)

edx %>% group_by(genres) %>% 
  summarise(n=n()) %>%
  head()
```


```{r}
edx <- edx %>% mutate(timestamp = as.POSIXct(timestamp, origin = "1970-01-01", 
    tz = "GMT"))
edx$timestamp <- format(edx$timestamp, "%Y")

colnames(edx)
```
Explore the QUALITATIVE FEATURES: genres, title

Importing required libraries to build a word cloud and understand which genre has the highest count amongst all.
```{r}
##created a data frame top_genr which contains each genre ( ex : Action, Comedy) . This can help to visualize which single genre has more ratings. Thus a word cloud is formed
top_genr <- edx %>% separate_rows(genres, sep = "\\|") %>%
  group_by(genres) %>%
  summarize(count = n()) %>%
  arrange(desc(count))

datatable(top_genr, rownames = FALSE, filter="top", options = list(pageLength = 5, scrollX=T) ) %>%
  formatRound('count',digits=0, interval = 3, mark = ",")
```

Similarly we can explore which title has the most ratings through a barchart
```{r}
library(kableExtra)
# the data frame top_title contains the top 20 movies which count the major number of ratings

top_title <- edx %>%
  group_by(title) %>%
  summarize(count=n()) %>%
  top_n(20,count) %>%
  arrange(desc(count))

# with the head function i output the top 5 

kable(head(edx %>%
     group_by(title,genres) %>%
     summarize(count=n()) %>%
     top_n(20,count) %>%
     arrange(desc(count)) ,
     5)) %>%
  kable_styling(bootstrap_options = "bordered", full_width = F , position ="center") %>%
  column_spec(1,bold = T ) %>%
  column_spec(2,bold =T) %>%
  column_spec(3,bold=T)
```

```{r}
#bar chart of top_title

top_title %>% 
  ggplot(aes(x=reorder(title, count), y=count)) +
  geom_bar(stat='identity', fill="red") + coord_flip(y=c(0, 40000)) +
  labs(x="", y="Number of ratings") +
  geom_text(aes(label= count), hjust=-0.1, size=3) +
  labs(title="Top 20 movies title based \n on number of ratings" , caption = "source data: edx set")
```
QUANTITATIVE FEATURES: UserId, movieId, timestamp

```{r}
edx %>%
  summarize(n_users = n_distinct(userId),
            n_movies = n_distinct(movieId))

```

Even if each row represents a rating given by one user to one movie, the number of unique values for the userId is 69878 and for the movieId 10677 : Both usersId and movieId which are presented as integer should be presumably treated as factors for some analysis purpose. Also, this means that there are less movies provided for ratings than users that rated them . If we think in terms of a large matrix, with user on the rows and movies on the columns, a challenge we face is the sparsity of our matrix. This large matrix will contain many empty cells. More over, we face a curse of dimensionality problem .These issues should be treated in the further analysis.
```{r}
# histogram of number of ratings by movieId

edx %>% 
  count(movieId) %>% 
  ggplot(aes(n)) + 
  geom_histogram( bins=30, color = "blue") +
  scale_x_log10() + 
  ggtitle("Movies") +
  labs(subtitle  ="number of ratings by movieId", 
       x="movieId" , 
       y="number of ratings", 
       caption ="source data : edx set") +
  theme(panel.border = element_rect(colour="black", fill=NA)) 
```


```{r}
# histogram of number of ratings by userId


edx %>% 
  count(userId) %>% 
  ggplot(aes(n)) + 
  geom_histogram( bins=30, color = "gold") +
  scale_x_log10() + 
  ggtitle("Users") +
  labs(subtitle ="number of ratings by UserId", 
       x="userId" , 
       y="number of ratings") +
  theme(panel.border = element_rect(colour="black", fill=NA)) 
```
3. METHOD : 
    3.1.Data transformation
Trying to build a matrix, as we  huge amount of data, the dcast , acast functions of the reshape2 and data.table packages are very time consuming and don’t allocate vectors of size more than 2.8G. Then, go further with the Matrix packages : Matrix and Matrix.utils which contain the sparseMatrix function. 
```{r}
RMSE <- function(true_ratings, predicted_ratings){
    sqrt(mean((true_ratings - predicted_ratings)^2))
  }
```

```{r}
# As in the Data exploration step,  usersId and movieId should be treat as factors for some analysis purposes. To perform this transformation  make a copy of edx set, since we want to keep unchanged our original training set.(edx)

edx.copy <- edx

edx.copy$userId <- as.factor(edx.copy$userId)
edx.copy$movieId <- as.factor(edx.copy$movieId)
```



```{r}
# Use the SparseMatrix function. The output is a sparse matrix of class dgcMatrix.
# In order to use this function, convert userId & movieId into numeric vectors.

edx.copy$userId <- as.numeric(edx.copy$userId)
edx.copy$movieId <- as.numeric(edx.copy$movieId)

sparse_ratings <- sparseMatrix(i = edx.copy$userId,
                         j = edx.copy$movieId ,
                         x = edx.copy$rating, 
                         dims = c(length(unique(edx.copy$userId)),
                                  length(unique(edx.copy$movieId))),  
                         dimnames = list(paste("u", 1:length(unique(edx.copy$userId)), sep = ""), 
                                        paste("m", 1:length(unique(edx.copy$movieId)), sep = "")))


# Now remove the copy od edx dataset which was created
rm(edx.copy)

#Analyse the first 10 users
sparse_ratings[1:10,1:10]
```


```{r}
#Convert the rating matrix into a recommenderlab sparse matrix
library(recommenderlab)
ratingMat <- new("realRatingMatrix", data = sparse_ratings)
ratingMat
```
To measure the similarity between users or between items, it is possible to use the following:
+ Minkowski Distance
+ Mahalanobis distance
+ Pearson correlation
+ Cosine similarity 
we will use the cosine similarity which is a measure of similarity between two non-zero vectors of an inner product space that measures the cosine of the angle between them.

Because of our huge amount of data, we calculate our similarity on the first 50 users for the visualization.



```{r}
#i calculate the user similarity using the cosine similarity

similarity_users <- similarity(ratingMat[1:50,], 
                               method = "cosine", 
                               which = "users")

image(as.matrix(similarity_users), main = "User similarity")



#Using the same approach, I compute similarity between  movies.

similarity_movies <- similarity(ratingMat[,1:50], 
                               method = "cosine", 
                               which = "items")

image(as.matrix(similarity_movies), main = "Movies similarity")
```




```{r}
set.seed(1)
Y <- irlba(sparse_ratings,tol=1e-4,verbose=TRUE,nv = 100, maxit = 1000)

# plot singular values

plot(Y$d, pch=20, col = "blue", cex = 1.5, xlab='Singular Value', ylab='Magnitude', 
     main = "Singular Values for User-Movie Matrix")

```


```{r}
# calculate sum of squares of all singular values
all_sing_sq <- sum(Y$d^2)

# variability described by first 6, 12, and 20 singular values
first_six <- sum(Y$d[1:6]^2)
print(first_six/all_sing_sq)
```


```{r}
first_12 <- sum(Y$d[1:12]^2)
print(first_12/all_sing_sq)
```


```{r}
first_20 <- sum(Y$d[1:20]^2)
print(first_20/all_sing_sq)
```


```{r}
perc_vec <- NULL
for (i in 1:length(Y$d)) {
  perc_vec[i] <- sum(Y$d[1:i]^2) / all_sing_sq
}

plot(perc_vec, pch=20, col = "blue", cex = 1.5, xlab='Singular Value', ylab='% of Sum of Squares of Singular Values', main = "Choosing k for Dimensionality Reduction")
lines(x = c(0,100), y = c(.90, .90))
```

```{r}
#Before performing Matrix Factorization method clear the used memory
invisible(gc())

# Matrix Factorization with parallel stochastic gradient descent

#iCreate a copy of training(edx) and validation sets to retain only userId, movieId and rating features. Also, rename the three columns.

edx.copy <-  edx %>%
            select(-c("genres","title","timestamp"))

names(edx.copy) <- c("user", "item", "rating")


valid.copy <-  validation %>%
  select(-c("genres","title","timestamp"))

names(valid.copy) <- c("user", "item", "rating")


#Use as.matrix function on edx.copy dataset
edx.copy <- as.matrix(edx.copy)
valid.copy <- as.matrix(valid.copy)


#write edx.copy and valid.copy tables on disk 
write.table(edx.copy , file = "trainset.txt" , sep = " " , row.names = FALSE, col.names = FALSE)
write.table(valid.copy, file = "validset.txt" , sep = " " , row.names = FALSE, col.names = FALSE)


#  data_file(): Specifies a data set from a file in the hard disk. 

set.seed(123) # This is a randomized algorithm
train_set <- data_file("trainset.txt")
valid_set <- data_file("validset.txt")



#Next step is to build Recommender object
r = Reco()


# First Step of Matrix Factorization :  Tuning Training Set

opts = r$tune(train_set, opts = list(dim = c(10, 20, 30), lrate = c(0.1, 0.2),
                                     costp_l1 = 0, costq_l1 = 0,
                                     nthread = 1, niter = 10))
opts
```


```{r}
#Second Step of Matrix Factorization : Train the recommender model
r$train(train_set, opts = c(opts$min, nthread = 1, niter = 20))

```


```{r}
#Third Step of Matrix Factorisation : Making prediction on validation set and calculating RMSE:

pred_file = tempfile()

r$predict(valid_set, out_file(pred_file))  
```


```{r}
# show first 10 predicted values
print(scan(pred_file, n = 10))
```


```{r}
#Perform prediction on valid_set
scores_real <- read.table("validset.txt", header = FALSE, sep = " ")$V3
scores_pred <- scan(pred_file)

#remove copies of training and validation sets
rm(edx.copy, valid.copy)

rmse_mf <- RMSE(scores_real,scores_pred)
rmse_mf  
```


```{r}
```
```{r}

```


